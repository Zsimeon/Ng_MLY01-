# 训练曲线

# 28、诊断偏差和方差：学习曲线

我们还未找到方法来估计误差能怎样影响可避免偏差和方差。我们通过估计最优的误差率和计算算法训练集和开发集误差来估计。
让我们讨论一个更有意义的技巧：画一个学习曲线。

一个学习曲线画出你的开发集误差和训练样本数量之间的关系。为了画出它，你应该使用不同尺寸的训练集来运行你的算法。
例如，如果你有1000个样本，你可以分散训练算法在100,200,300，...，1000个样本上。接着你可以画开发集误差
和不同训练训练集尺寸之间的图。

随着训练集尺寸的增加，开发集误差应该下降。

我们通常会有一些“渴望的误差率”，我们希望我们的学习算法最终得到这样的误差率。例如：

- 如果我们期望人类级别的性能，那么人类误差率可以使“渴望的误差率”。

- 如果我们的学习算法服务于一些产品（例如分类猫咪图片），我们可以有一个直觉来关注需要什么样的性能等级来
给使用者一个很棒的体验。

- 如果你曾在一个重要的应用上工作过很长时间，那么你可能会有一个直觉关于下一个季度或下一年你应该能达成的进度。

给你的学习曲线添加渴望的性能等级。

你可以用肉眼来推断这个红色的“开发误差”曲线来猜测通过添加更多的数据你会得到的误差率和渴望的性能等级会靠的多近。

但是如果开发曲线误差趋向平稳（疲惫了），那么你能够立即判断添加更多的数据不能使你达到你的目标

看学习曲线可以帮助你避免花费几个月的时间收集两倍的训练数据，才发现他并不能有帮助。

这个方法的一个坏处就是如果你只看开发误差曲线，可能很难确切的推测和预测如果你有更多的数据这个红色曲线会怎样画。
有一种额外的画图可以帮助你估计添加更多数据的影响：训练误差。


# 29、画训练误差

你的开发集（和测试集）误差在训练集尺寸上升的时候应该降低。但是你的训练集误差通常会随着训练尺寸增加而增加。

让我们用一个例子来说明这个影响。假设你的训练集只有两个例子：一个猫猫图片和一个非猫图片。那么对于学习算法能轻松的
“记忆”训练集中的两个例子，并得到0%的训练集误差。即使有一个或者两个样本都被错误标记了，对于算法也能很轻松的记忆两个标签。

现在，假设你的训练集有100个样本。可能只有很少的样本是被错误标记的，或者模糊不清的---一些图片很模糊，所有即使是
人类也不能 分辨这是不是猫。可能学校算法仍能“记忆”大多数的训练集，但是他现在更难获得100%的准确率。通过增加训练集
从2个到100个样本，你会发现训练集准确率会轻微的下降。

最后，假设你的训练集有10000个样本。在这种情况下，对于算法来说，更难完美的适应所有的10000个样本，特别如果一些
是模糊不清的或错误标记的。因此，你的学习算法会再训练集上表现得更差。

给我们的早期图像添加一个训练集误差曲线。

你可以看到蓝色的“训练误差”曲线随着训练集尺寸增加而增加。进一步说，你的算法通常在训练集上表现得比在开发集上更好；
因此这个红色测试误差曲线通常位于蓝色训练误差曲线上面。

让我们接下来讨论怎么解释这些图像。


# 30、解释学习曲线：高偏差

假设你的开发误差曲线看起来像这样

我们之前说过，如果你的开发曲线平台，你不可能仅通过添加数据就能获得想要的性能。

但是很难来精确的搞清楚图推断红色开发曲线会变成什么样。如果开发集很小，你会更不确定因为曲线会有噪声。

假设我们给这个图添加训练误差曲线得到下图。

现在，你能完全的确定添加数据不会有效果。为什么会这样？想一想我们的两个观察：

- 随着添加更多的训练数据，训练误差只会变得更差。因此，这个蓝色训练误差曲线只能够保持不变或变高，因此他只能够
离期望的性能等级（绿线）更远

- 红色开发误差曲线通常比蓝色训练误差要高。因此，几乎没办法通过添加更多的数据来使红色曲线下降到期望的性能等级
当训练误差比渴望的性能等级还要高的时候。

在同一个图里检查开发误差曲线和训练误差曲线允许我们更自信的退出开发误差曲线。

假设，对于讨论的目的，期望得到的性能是我们估计的最优误差率。这个图是标准的“教科书”例子关于学习曲线有高可避免偏差时
应该是什么样：最大的训练集尺寸--可以假定考虑到我们有的训练数据--在训练误差和渴望性能之间有很大的间隔，按时大的可避免偏差。
进一步说，训练集合开发集之间的间隔很小，有很小的方差。

先前说过，我们测量训练和开发集误差只能在图像最右边的点，使用了所有的可获得的训练数据。画出全学习曲线给了我们
更多可理解的在不同训练集尺寸的关于算法性能的图片。

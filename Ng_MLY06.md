# 31 解释学习曲线：其他情况

考虑这个学习曲线：

这个曲线显示了高偏差，高方差，还是都是？

蓝色的训练误差曲线相关性很低，红色的开发误差曲线比蓝色训练误差要高的多。因此，偏差很低但方差很高。
添加更多的数据可能会帮助减少开发误差和训练误差之间的间隔。

现在，考虑这个：

这一次，训练误差很大，比期望的性能等级要高的多。开发误差也比训练误差大得多。因此，你有显著的偏差和显著的方差。
你需要找到个方法来在你的算法中减少偏差和方差。


# 32、绘制学习曲线

假设你有很小的100个样本的训练集。你使用随机的选择10个样本、20个样本、30、直到100的样本集来训练你的算法，
每次迭代增加10个样本数。你然后使用这些10个数据点来绘制你的学学习曲线。你可能会发现这个曲线看起来有点轻微的
噪声（意味着这个值比预期的要高或低），在小训练集尺寸的情况下。

当训练只有10个随机选择的样本时，你可能会不幸的有一个特定的“坏”的训练集，例如一个由很多不清楚的或误标记的样本。
或者你可能有幸的得到一个特定的“好”的训练集。有一个小训练集意味着开发和训练误差可能是随机的波动。

如果你的机器学习应用严重的歪斜向一个层级（例如一个猫分类器任务的负面样本数比正面样本数要高的多），或者如果
有很多的层级数（例如识别100个不同的动物种类），那么选择一个特别的“不典型的”或坏的训练集的可能性是很大的。
例如，如果80%的样本是负面样本（y=0），只有20%的是正面样本（y=1），那么有可能训练集中的10个样本只包含负面样本，
因此使算法学习一些有意义的事是很难的。

如果训练曲线的噪声使得它很难看到真的趋势，这里有俩解决方法：

- 相较于仅在10个样本上训练一个模型，选择一些（3-10个）不同的随机选择的有10个样本训练集，从你的初始的100个的集中采集。
训练在这些集中训练不同的模型，计算这些结果模型的训练和开发集误差。计算并绘制平均训练误差和平均开发集误差。

- 如果你的训练集歪斜向一个类别，或者他有很多的种类，选择一个“均衡的”子集而不是从100个中随机选10个训练样本。
例如，你能够确定2/10的样本是正面样本，8/10的样本是负面样本。更通常的说，你能够确保选择的样本对于每一个类别所占的
比例数尽可能的和原始训练集的比例数一致。

我不会操心这些技巧中的任何一个，除非你已经尝试绘制学习曲线和推断出这个曲线有太多噪声而不能看到隐藏的趋势。
如果你的训练集很大--超过10000个样本--你的种类分布也不是很倾斜，你可能并不需要这些技巧。

最后，绘制学习曲线可能是计算昂贵的：例如你可能需要训练10个模型有1000、2000直到10000个样本。训练小数据集的模型比
训练大数据集的模型要快的多。因此，相较于均匀的线性规模的增加训练集尺寸，你能够训练模型通过1000、2000、4000、6000
和10000个样本。这也会给你一个清楚的对于学习期限的趋势的感觉。当然，这个技巧只在计算所有额外的模型时耗费很显著的情况
下有重大作用。
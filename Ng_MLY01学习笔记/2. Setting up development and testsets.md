# 5、你的开发设测试设置

让我们回到更早的猫咪图片识别例程：你运行一个手机app，使用者上传许多的不同东西的图片到你的app上。你想要自动的寻找猫咪图片。

你的团队通过网站上传的猫咪图片得到了一个大规模的训练集（正面样例）和非猫图片（反面样例）。他们把数据集分为
70%和30%的训练集和测试集。使用这些数据，他们建立了一个猫咪检测器并且在训练和测试集上运行的很好。

但是，当你把这个分类器布置到手机app上时，你发现性能非常差。

发生了啥？

你搞清楚了构成你的训练集的网站图片和这些使用者上传的图片看起来不同：使用者上传的图片是用手机拍的，可能是
低分辨率的，模糊的，暗的。然而你的测试集训练集是用网站上的图片构成的，你的算法并不能够很好地推广到你关心的真实的分布
（即手机图片）上。


在现代的大数据时代之前，使用随机的70%和30%分隔来形成你的训练和测试集是一种常用的机器学习规则。
这种方法实际能够工作，但是在越来越多的应用场景里，尤其训练数据分布（上文提到的网站图片）
和你实际关注的分布（即手机图片）不同的时候，这是一个坏主意。bad idea!

我们常常定义;
- *训练集*：你训练你的学习算法的地方
- *Dev（development）set 开发集*： 用来调试参数，选择特征，和做其他的关于学习算法的决定。
有时也叫做hold-out cross validation set。
- *测试集*： 用来评估算法性能，但不据此做任何学习算法或参数相关的决定。


一旦你定义一个开发集和测试集，你的团队会尝试很多的点子，例如不同的学习算法参数来看哪个工作的最好。
开发和测试集允许你的团队来快速的看你的算法工作的怎样。

换句话说，开发和测试集的目的就是指挥你的团队做机器学习系统的最重要的改变。

因此，你应该做如下工作：
	选择开发和测试集来反映你想要得到的数据和你想要做好的方面。

换句话说，你的测试集应该不仅是简单的可获得数据的30%，尤其是如果你预想你未来的数据（手机图片）和你训练集中的图片（网站图片）不同的时候。

如果你还没上线你的手机app，你应该还没有使用者，这可能会导致你不能够得到能准确的反映你未来想做的方面的数据。
但你依然能够尝试去估计近似这些值。例如，请你的朋友来用手机拍摄猫咪的图片再发给你。一旦你的app上线了，
你能使用用户数据来更新你的开发和测试集。


如果你真的没有任何方法来得到数据近似你未来可能得到的数据，可能你需要从使用网站数据开始。
但你应该认识到这个风险可能会导致系统不能有很好地适应性。

这需要判断来决定应该投入多少在发展伟大的开发和测试集上。但是不要假定你的训练分布和你的测试分布是一样的。
尝试采取能够反映你最终想要在其上完美运行的测试样例，而不是其他任何你偶然获得的数据来训练。


# 6、你的开发和测试集应该来着同一分布

你基于你的最大的市场，把你的猫咪app图像数据分成了四个区域：i)美国；ii)中国；iii)印度，iv)其他。为了得到开发和测试集，
假定我们把美国和印度作为开发集，中国和其他作为测试集。换句话说，我们能够随机的分配这些部分的两个到开发集，另两个到测试集，对不对？

错！！！！！

一旦你确定了开发和测试集，你的团队会聚焦于提高开发集性能。因此，开发集应该能够反映你最想提高的任务：
在所有的四个地区都表现得好，而不是两个。

有不同的开发和测试集还有第二个问题：可能你的团队会再开发集上运行的很好，在测试集上表现得很差。我曾经看过很多这些导致的挫折和无用功。
避免这些发生在你身上。

作为一个例子，假设你的团队开发一个系统在开发集上运行的很好但在测试集上不行。如果你的开发和测试集来自相同的分布，
你可以清楚的诊断出哪里有错误：你过度适应开发集了（overfit）。这明显的解决方法是获取更多的开发集数据。

但是如果开发和测试集来自不同分布，因此你的选项就不够清晰。好几种东西都有可能出错。

1. 你过适应开发集。
2. 测试集比开发集更难学习。（The test set is harder than the dev set）
因此你的算法可能如预料的工作那么好，但是不可能会有更显著的未来的提升。
3. 测试集不是必要的harder，而仅仅只是和开发集不同。因此在开发集上运行的好而不能在测试集上运行的好。
如果这样的话，许多的你提升开发集性能的工作只是无用功。


机器学习应用上面的工作已经足够难了。拥有不匹配的开发和测试集导致了额外的不确定性关于是否提升开发集分布上的性能还是提示测试集性能。
不匹配的开发和测试集使搞清楚哪些在工作哪些不在工作变得更难。也使下一步工作的优先次序也变的更难安排。

如果你在第三方准则问题工作，他们的开发者肯会有特定的开发和测试集来自不同的分布。幸运的是，而不是技巧的，
如果开发和测试集来自相同的分布，会对你的性能有一个很大的影响。训练在一个分布和在其他上适应的很好，
这在开发学习算法上是一个重要的研究问题。但是如果你的目标是在一个特定的机器学习算法上运行而不是做研究，
我推荐你尝试选择开发集和测试集来自同一分布。这会是你的团队更有效率。


# 7、开发和测试集应该多大？

开发集应该足够大来检测你尝试的算法的不同。例如，如果分类器A有90%的准确率，分类器B有90.1%的准确率，
那么有100个例子的开发集就不能够检测出这0.1%的不同。和其他我曾见过的机器学习问题相比，一个100个例子的开发集是
很小的。开发集通常的大小是1000到10000个样例。如果有10000个样例，你能够有很好地机会来检测0.1%的提升。

对于成熟的和重要的应用--例如，广告、网页搜索，和产品推荐。我曾经看过一些团队非常有动力去增加甚至0.01%提升，
因此这对公司的效益有着直接的影响。从这点来看，开发集可以有比10000更大的数据，为了得到甚至更小的提升。

那么测试集的大小又该怎样呢？它应该足够大来给你的系统的性能有一个整体的高度自信。一个流行的启发性的方法是使用
30%的数据给你的测试集。当你有适度的样例数据的时候这能运行的很好--100到1000样例。但是在大数据时代，我们常遇到的机器学习问题
都有着超过十亿的样例，数据分给开发集和测试集的部分已经开始减少，恰在此时开发测试集中样例的绝对值开始增加。
不用超过你需要评估你的算法性能的过度大的开发和测试集。


# 8、为你的团队进行优化而创建一个单变量评估策略

分类准确率是一个单变量评估准则的例子：你在开发集（或测试集）上运行你的分类器，来得到一个单变量关于样例中的哪些部分被正确分类。
根据这个准则，如果分类器A有97%的准确率，分类器B有90%的准确率，那么我们判断分类器A是更高级的。

相反，Precision and Recall不是一个单变量评估准则：给了两个数来评价呢的分类器。拥有多变了评估准则使它更难来对比算法。
假设你的算法性能如下所示：

	Classifier    |  Precision   |Recall
	-------------------------------------
	A             |  95%         | 90%
	-------------------------------------
	B             |  98%         | 85%

这里，哪个分类器都不是明显的更好地，因此不能立刻指导你选择一个。

在开发过程中，你的团队会尝试大量的关于算法结构、模型参数、特征选择的点子，等等。使用一个准确率的单变量评估准则
能够允许你根据模型在这个准则下的性能分类你所有的模型，并快速的决定哪个工作的最好。

如果你真的关注Precision and Recall，我推荐使用一种标准的方法来把它们组合成单变量。例如，可以去precision和recall的平均值，
来以单变量结束。或者你可以计算“F1 score”，这是一种改进的计算它们的平均值的方法，并比单纯的取均值要更好。

	Classifier    |  Precision   |Recall  |  F1 score
	---------------------------------------------------
	A             |  95%         | 90%    |    92.4%
	---------------------------------------------------
	B             |  98%         | 85%    |    91.0%

使用单变量评估准则加速你从大量分类器中选择的能力。它给出了在所有的分类器中的一个清楚的偏向等级，而且有一个清楚的工作导向。

作为最后一个例子，假设你单独的跟踪你的猫咪分类器在四个主要市场的准确率：美国、中国、印度和其他。这给出了四个准则。
通过对着四个数据选择一个平均值或者权重平均值，你最终得到一个单变量准则。选择均值或者权重均值是一种最常用的把多变量整合成单变量的方法。



# 9、 优化和满意度准则

这里有一种其他的方式来整合多变量准则

假设你关心学习算法的准确率和运行时间。你需要从这三个分类器中选择：

	Classifier    |  Precision   |Recall   |  
	-----------------------------------------
	A             |  95%         | 80ms    |    
	-----------------------------------------
	B             |  98%         | 95ms    |    
	-----------------------------------------
	C             |  95%         | 1500ms  |

看起来把准确率和运行时间通过一个公式整合成单变量是不自然的，例如：
	Accuracy - 0.5 * RunningTime

这里你可以如下做来替代：首先，确定什么是一个可接受的运行时间。让我们设定任何小于100ms的运行时间都是可以的。
然后，最大的准确率，把你的分类器服从于时间准则。这里，运行时间是一个“满意度准则”--你的分类器仅需要在这个条件下足够好。
即它最多运行100ms。准确率是“优化指标”。

如果你在N个不同的准则里琢磨不定，例如模型的二进制文件的大小（这对于手机app很重要，因为用户不想下载很大的apps），运行时间和准确率，
你应该考虑设置N-1个准则为满意度准则。例如，你简单的需要他们符合一个确定的值。然后定义最后的哪个为最优化指标。例如，
设定一个阈值给能接受的二进制文件大小和运行时间，并努力来优化给定条件下的准确率。

最后举个例子，假设你建造一个硬件设备使用麦克风来听用户说出的确切的“wakeword”，然后启动系统。例如Amazon的Echo听到“Alexa”，
苹果的Siri听到“Hey Siri”，安卓听到“Okay Google”，百度听到“Hello Baidu”。你关心所有的错误积极率（False positive rate）
--当没人说wakeword的时候系统唤醒的频率和错误负面率（false negative rate）--当有人说wakeword的时候它没有启动的次数。
对于这个系统，一个合理的目标是最小化false negative rate（最优策略），使得每24小时的操作不超过1个False positive。

一旦你的团队一致最优化评估准则，他们能够创造更快的进步。


# 10、开发集和度量标准加速迭代

很难提前了解什么方法会对一个新问题运行的很好。甚至有经验的机器学习研究人员也常常在他们发现一些满意的方法之前尝试很多种点子。
在构建一个机器学习系统的时候，我经常会：
1. 开始时从一些怎样构造系统的方法入手
2. 用代码实现想法
3. 用实验来判断我这个想法运行的咋样。（通常我的第一个想法不成功！）基于这些学习到的东西，继续产生别的想法，保持循环。

这是一个不停迭代的过程。你越快的持续这个循环，你越容易得到成果。这就是为什么使用开发和测试集以及判断准则是如此的重要：每当你尝试一次想法，
对于你的开发集的性能的评价会使你更快的绝地你是否朝着正确的方向前进。

相反，假设你没有特定的开发集和评价准则。每次你的团队开发一个新的猫咪分类器的时候你必须把它合并到你的app中，并且花费一些时间
来得到你的新分类器是否有所提高。这会是难以想象的慢！同时，如果你的团队提高分类器准确率从95%到95.1%，在使用这个app的时候，
你可能不能够检测出这0.1%的提升。

然而很多你的系统的提升会逐渐的由这些0.1%来累积来实现。使用一个开发集和衡量准则会使你能够快速的检测到哪些想法是有效的带给你小小的提升，
并因此使你快速的决定哪些想法该继续修改下去，哪些是该舍弃的。


# 11、什么时候去修改开发集测试集和衡量准则

当开始一个新的工程的时候，我尝试快速选择开发和测试集，因为这能给团队一个被很好定义的目标去为之努力。

我通常会再一周之内让我的团队去想出一个最初的开发和测试集，和一个初始的衡量准则。往往更应该去做一个不是最完美的但是能快速开始的，
而不是过度思考。但是这个一周的时间线不能应用于成熟的应用。例如，反垃圾邮件是一个成熟的深度学习英语。我曾经见过有的团队在已经成熟的系统上
花费几个月的时间来得到更好地开发和测试集。

如果你后来发现你的初始开发和测试集或衡量准则错过了特征，立马快速改变他们。例如，如果你的开发集和衡量准则认为分类器A比B好，
但是你的团队成员认为B实际上比较适合你的产品，那么这可能意味着你需要改变你的开发和测试集或者你的衡量准则。

有三个主要的开发集和衡量准则给分类器A评分更高的原因：
1. 你实际需要表现得好的分布和开发和测试集不同。
假设你的初始开发和测试集主要是成年猫咪图片。你使用你的猫咪app，发现使用者比预期的上传更大量的小猫咪图片。因此，
开发和测试集分布就不能代表你实际应该表现得好的分布。在这个案例中，更新你的开发和测试集来使其变得更有代表性。

2. 你过拟合开发集了

反复的在开发集上评估想法导致你的算法逐渐的在开发集上“过拟合”。当你在开发的时候，你会在测试集撒花姑娘评估你的系统。
如果你发现你的开发集性能比你的测试集性能搞的多，这意味着你过拟合了开发集。在这种情况下，更新开发集。

如果你需要跟进你团队的进程，你也可以定期地评估你的系统--一周一次或一个月一次--在测试集上，但是不要使用测试集来做任何关于算法的决定。
包括是否回滚但上一周的系统。如果你这么做了，你会开始过拟合测试集，并且不再能够得到一个完全无偏差的系统性能评估
（但这是你写研究文章时需要用到的，或者使用这个准则来做重大的商业决定）。

3. 衡量准则衡量了其他东西而不是这个工程需要去优化的东西

假设你的猫咪应用，你的衡量准则使分类准确率。这个衡量准则现状评级分类器A比分类器B好。但是假设你尝试这两个算法，并发现A偶尔会允许色情图片闪过。
即使A是更准确的，特殊场合下的色情的图片带来的影响是他的性能不能被接受。你需要做什么呢？

这里，衡量准则不能辨别算法B是否实际上比A更适合你的产品。因此，你不能再相信这个衡量准则来选择最好的算法。是时候去改变这个苹果准则了。
例如，你能够改变准则来对允许色情图片增加惩罚。我强烈建议选择一个新的准则并使用这个新的准则来确切的定义一个新的团队目标，而不是在没有
一个值得信赖的准则的时候长时间运行，并重新回到手动的在分类器中选择。

在一个工程中改变开发和测试集或更新衡量准则使很常见的事。使用一个初始的开发测试集和衡量准则帮你快速的迭代。如果你一旦发现这个开发和测试集或衡量
准则不再指引你的团队前往正确的方向，这不是什么大问题！仅仅改变他们并确保你的团队知道这个新方向。


# 12、结语： 设置开发和测试集

- 从能够反映你未来想要得到并在其上运行的很好的数据同一分布中选择开发和测试集。这不一定是和你的训练数据的分布一样。
- 如果可能的话从相同的分布中选择开发和测试集。
- 为你团队的优化选择单变量评估衡量准则。如果你关心的是多目标的，考虑把他们整合起来成单个公式（例如平均多误差准则）或者定义满意和最优化准则。
- 机器学习是高度迭代的过程：在找到你满意的那个方法之前你可能尝试许多中的方法。
- 有开发和测试集和单变量评估衡量准则帮助你快速的评估算法，并更快地迭代。
- 当开始一个新的应用时，尝试快速建立一个开发测试集和一个衡量准则，最好在一周之内。在一个成熟的应用上这时间可以适度更长。
- 老的启发式的70%、30%的训练、测试分隔不再适用于当你有许多的数据的时候；开发和测试集能够比30%的数据要小得多。
- 你的开发集应该足够大来检测你的算法带来的有意义的准确度改变，但也不需要太大。你的测试集应该足够大来给你一个确定的对于你系统的最后评估。
- 如果你的开发集和衡量准则不再指引你的团队向正确的方向，赶紧改变他们：(i)如果你过拟合了开发集，获得更多的开发集数据。
(ii)如果你关心的真实分布和开发测试集的分布不同，获取一个新的开发测试集数据。(iii)如果你的衡量准则不再衡量对你而言最重要的东西，改变这个准则。



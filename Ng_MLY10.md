# Debugging inference algorithms

## 44、最优化验证测试

假设你构建一个语音识别系统。你的系统通过输入一个语音片段A，并对每一个可能的输出语句S计算一些得分Score_A(S)。例如你可能会尝试估计Score_A(S)=P(S|A)，给定输入语音为A的时候，正确输出转录是语句S的可能性。

给定一个方法来计算Score_A(S)，你仍必须去找英语语句S，并使其最大：

	输出 = arg max_S Score_A(S)

你怎么计算上面的“arg max”呢？如果英语有50000个词，那么对于长度为N的语句有(50000)^N种可能性--
太多了而不能全部枚举出来。因此，你需要应用一个近似搜索算法，来尝试找到S的值来最优化（最大化）Score_A(S).
一个样例搜索算法是“beam search（定向搜索）”，在搜索过程中保持只有K个顶端的候选项。（为了本章的目标，
你不用理解定向搜索的细节）。向这样的算法不能保证能找到S的值使得Score_A(S)最大。

假设一个语音片段A记录了某人说“我爱机器学习”。但是你的系统输出不正确的“我爱机器人”。有两种可能性使你的算法出错。

1. *搜索算法问题*。近似搜索算法（定向搜索）错误的找打S的值来最大化Score_A(S)。

2. *目标（得分函数）问题*。我们的对于Score_A(S)=P(S|A)的估计是不正确的。具体地说，我们的对于Score_A(S)的
选择不能识别“我爱机器学习”是正确的转录。

取决于什么是导致失败的原因，你应该不同的把你的精力最优排序。如果问题是第一点，你应该在改进搜索算法上努力。
如果是第二个原因，你应该在估计Score_A(S)的学习算法上哪努力。

面对这种情况，一些研究者会随机的决定工作在搜索算法；其他的uhi随机的找一个更好地方法来学习Score_A(S)的值。
但是除非你知道什么是导致错误的原因，你的努力是无效的。你应该怎样更系统的决定在哪方面努力呢？

把S_out作为转录输出（“我爱机器人”）。使S^\*为正确的转录(“我爱机器学习”）。为了明白是上面提到的1还是2
导致了这个问题，你能开始*最优化验证测试*：首先，计算Score_A(S^*)和Score_A(S_out)。然后检查是否Score_A(S^*)>Score_A(S_out)。有两种可能性：

1. Score_A(S^\*)>Score_A(S_out)

在这种情况下，你的学习算法正确的给定S^\*有一个比S_out更高的得分。这告诉你你的近似搜索算法不能选择S的值来最大化
Score_A(S)。这种情况下，最优化验证测试告诉你你有一个搜索算法问题，并应该关注这个问题。例如，你应该尝试增加定向搜索的
beam宽度。


2. Score_A(S^\*)<Score_A(S_out)

在这种情况下，你知道你计算Score_A(.)的方法有问题：不能给正确输出S^\*比不正确的S_out一个严格地更高的值。
最优化验证测试告诉你你有了一个目标（得分）函数问题。因此，你应该关注与改进你怎么学习或者怎么对于
不同的语句S近似Score_A(S^)。

我们的讨论关注于单一样例。为了在实践中应用最优化验证测试，你应该检测你的开发集的错误。对于每一个错误，
你应该测试是否Score_A(S^\*)>Score_A(S_out).开发集中的每一个导致不正确的样例会被标记为有最优算法导致
的误差。每个使得Score_A(S^\*)<Score_A(S_out)计数一次，根据你计算Score_A(S)的方法作为一个错误。

例如，假设你发现95%的误差是由于得分函数Score_A(S)导致，只有5%由于最优化算法导致。现状你知道不论你
怎么改进你的最优化过程，你会只能消除仅仅5%的误差。因此，你应该相反关注与改进你怎么估计Score_A(S).


## 45、最优化验证测试的通常形式

你能应用最优化验证测试当，给定一些输入x，你知道怎么计算Score_x(y)，对于一个输入x怎样的映射y是好的。
进一步说，你使用一个近似算法来尝试找到arg max_y Score_x(y)，但是怀疑搜索算法有时不能找到最大值。在我们前面
提到的语音识别例子中，x=A是一个语音片段，y=S是输出的转录文本。

假设y\*是“正确”输出，但是算法输出y_out。那么关键测试时测量是否Score_x(y\*)>Score_x(y_out)。如果这个不等于
成立，那么我们把错误归咎于最优化算法。相反，我们归咎于计算Score_x(y)。

再看一个例子。假设你构建一个汉语到应用的机器翻译系统。你的系统通过输入中文语句C，来对每一个可能的翻译E
计算一些Score_C(E)。例如，你可能使用Score_C(E)=P(E|C)，在给定输入语句C的情况下翻译是E的可能性。

你的算法翻译语句通过尝试计算：

	Output = arg max_E Score_C(E)
然而，所有的可能英语语句E太大了，因此你依据一个探索的搜索算法。

假设你的算法输出不正确的翻译E_out而不是正确的翻译E\*。那么最优化验证测试会让你计算是否
Score_C(E^\*)<Score_C(E_out)。如果这个不等式成立，那么Score_C(.)正确的识别E\*是一个比E_out更高级别的输出；
因此，你需要把这个误差归类于近似搜索算法。相反，你把这个错误归于对Score(.)的计算。

这在AI中是一个很常用的“设计模式”，首先学校一个近似得分函数Score_x(.)，然后使用一个近似最大化算法。
如果你能有认出这种模式，你会能使用最优化验证测试来理解你错误的源头。


## 46、强化学习样例

假设你使用及其学习来教直升飞机飞行复杂的演习。这有一个计算机控制的直升飞机在引擎关闭的情况下
进行着陆的时间流照片。

这叫做“自动选择”演习。允许直升机在他们的引擎意外的失效时着陆。人类飞行员进行这种演习作为他们训练的一部分。
你的目标是使用学习算法来飞一个直升飞机通过一个轨道T并最终安全着陆。

为了应用强化学习，你必须开发一个“奖励函数”R(.)，这给出一个分数来测量每一个可能的轨道T有多好。例如，
如果T导致了直升飞机毁坏，那么可能奖励是R(T)=-1000--一个巨大的负奖励。一个轨道T导致了安全着陆，可能会有一个
正面的R(T)，并随着降落有多么平滑来有一个确切的分数。奖励函数R(.)通过量化想要的不同的轨道T，
典型的被手动选择。必须平衡降落有多么颠簸，是否降落在精确的想要的落点，对于乘客这个降落有多难受，等等。
设计一个好的降落函数不容易。

给定一个奖励函数R(T)，强化学习的工作时控制直升飞机来使其获得max_T R(T).然而，强化学习算法做出了许多近似，
可能不能成功的获得这个最大值。

假设你取了一些奖励R(.)并运行了你的学习算法。然而，他的性能表现得比人类飞行员差的多--奖励很颠簸，并看起来
比人类飞行员要不安全的多。你怎么辨别强化学习算法中哪出错了--其通过尝试实验一个轨道并获得了max_T R(T)--或者
这个错是奖励函数--其尝试测量尽可能好的平衡旅程颠簸和降落点准确。

为了应用最优化验证测试，是T_human作为人类飞行员取得的轨迹，T_out为算法取得的轨迹。通过我们上面描述的，
T_human是比T_out更高级的轨道。因此，关键测试是如下：
能确保R(T_human)>R(T_OUT)吗？

结果1：如果这个不等式成立，那么奖励函数R(.)正确的评分T_human比T_out更好。但是我们的强化学习算法发现了
差的T_out。这暗示努力改进我们的强化学习算法是很值得的。

结果2：不等式不成立：R(T_human)<R(T_OUT)。这意味着R(.)分配一个更差的得分给T_human，即使他是更好地轨迹。
你应该努力改进R(.)来更好的捕捉好的着陆的平衡。

许多机器学习应用有这个使用一个近视搜素算法来最优化一个近似得分函数Score_x(.)的“模式”。有时候，
没有特定的输入x，这仅仅降低了Score(.)。在我们上面的例子中，得分函数时奖励函数Score(T)=R(T)，
最优化算法是强化学习算法尝试处理一个好的轨道T。

这个和上一个样例的不同点是，不是要比较一个“最优”输出，你而是要比较人类水平的性能T_human。我们假定
T_human要好的多，即使不是最优的。通常来说，只要你有一些y\*（这个例子中，T_human）是一个比你现在的学习算法
的性能更好地输出--即使不是最优输出--那么最优化验证测试能够指示是否更应该关注提升最优算法还是得分函数。
# Bias and Variance

## 20、偏差和方差：误差的两个大来源

假设你的训练、开发和测试集都来自相同的分布。那么你需要一直尝试来获得更多的训练数据，因为这样能够提升性能？对吗？

尽管有更多的数据没啥损害，不幸的是这不会像你想的那样一直有帮助。获得更多的数据可能是一种浪费时间的工作。因此，
什么时候是应该添加数据，什么时候不去烦恼呢？

机器学习里有两种主要的误差源：偏差和方差。理解他们会帮助你决定是否添加数据，或其他策略来提高性能，是对时间的良好应用。

假设你想构建一个有5%误差的猫咪识别器。现在，你的训练集有15%的错误率，你的开发集有16%的错误率。在这种情况下，添加数据
可能不会有太大帮助。你应该关注其他改变。实际上，给你的训练集添加更多的样本只能使你的算法更难在训练集上表现得好。
（在后续章节我们会说这是为什么）

如果你的训练集误差率为15%（或85%的准确率），但你的目标是5%的错误（95%的正确率），那么第一个需要解决的问题时提高你的算法
在训练集上的性能。你的开发和测试集性能通常比你训练集性能要差。因此如果你的算法在已经获得的数据上有85%的准确率，
那么不可能在没见过的数据上获得95%的准确率。

假设你的算法在开发集有16%的误差（84%的准确率）。我们把这16%的误差分为两部分：

- 首先，算法在训练集上的误差。在这个例子中，是15%。我们认为这是不正是的算法的偏差。

- 其次，算法在开发（测试）集上比训练集差多少。在这个例子中，开发集比训练集差了1%。我们把这认为是算法的方差。

学习算法的一些改变能够解决第一部分的误差--偏差--和提高他在训练集上的性能。一些改变解决第二部分--方差--并
帮助他能从训练集到开发集和测试集上推演的更好。为了选择最有前景的改变，搞清楚这两个误差哪个更需要去解决是
难以置信的有用。

开发好的偏差和方差的指导会帮助你选择有效的你的算法的改变。

## 21、偏差和方差的例子

考虑到我们的猫咪识别任务。一个“理想的”分类器（例如人类）可能会在这项任务上德大接近完美的性能。

假设你的算法性能如下：

- 训练误差=1%

- 开发集误差=11%

这有什么问题呢？应用前面章节的定义，我们估计偏差为1%，方差为10%（=11%-1%）。因此，他有很高的方差。这个分类器有很低的训练
误差牡丹石他不能推广到开发集。这也叫做过拟合。

现在考虑这个：

- 训练误差=15%

- 开发集误差=16%

我们估计偏差为15%，方差为1%。这个分类器适应训练集很差，15%的误差，但是他在开发集上的误差比在训练集上的要好很多。
因此这个分类器有很大的偏差，但是低的方差。我们说这个算法是欠拟合。

那么，考虑这个：

- 训练误差=15%

- 开发集误差=30%

我们估计偏差为15%，方差为15%。这个分类器有高偏差和高方差：它在训练集上表现得很差，因此有高偏差，在开发集上甚至更差，
所以他有高方差。过拟合和欠拟合术语在这里很难适用因为这个分类器同时的过拟合和欠拟合。

最后，考虑一下这个：

- 训练误差=0.5%

- 开发误差=1%

这个分类器表现得很好，因为他有低的偏差和低方差。祝贺能获得这么好的性能！


## 22、和最佳误差率对比

在我们的猫咪识别例子中，能够被一个最优的分类器所获得的理想的误差率，是接近于0%的。一个人类看着这些图片如果这些图片
一直都有一个猫咪的话是能够识别的；因此，我们能够期望机器能干的同样好。

其他问题更难。例如，假设你构建一个语音识别系统，并发现14%的语音片段有很多的背景音或者是非常难以理解的，甚至人类
都听不懂在说什么。在这种情况下，甚至最好最好的语音识别系统也会有14%左右的误差。

假设在这个语音识别问题中，你的算法获得了：

- 训练误差=15%

- 开发误差=30%

训练集性能以及很接近最优的误差率14%了。因此，并没有足有的提升空间给偏差或者说训练集性能了。然而，这个算噶不能
在开发集上推广的很好；因此对于方差导致的误差有充足的提升空间。

这个例子和前面章节的第三个例子很相似，同样有15%的训练集误差，30%的开发集误差。如果最好的误差率是0%，那么
训练误差的15%留下了充足的孔家来提升。这说明减少偏差的改变可能是有利的。但是如果最优的误差率是14%，那么
相同的训练集性能告诉我们对于这个分类器的偏差我们有很少的空间来提升。

对于最优误差率远大于0的问题来说，有更详细的一个算法误差的分析。继续我们上面的语音识别例子，整体30%的开发集误差
能够如下分解（相似的分析能够应用给测试集误差）：

- *最优误差率（不可避免的偏差）*：14%。假设如我们决定的那样，世界上最好的语音系统仍有14%的误差。我们能够把
这个认为是算法偏差的“不可避免”的部分。

- 可避免的偏差：1%。这是计算训练集误差和最优误差之间的差值。

- 方差：15%。开发集误差和训练误差之间的差值。

为了把这个和我们之前定义的联系起来，偏差和可避免的偏差和下面相关：
	偏差 = 最优误差率（“不可避免的偏差”） + 可避免的偏差

“可避免的偏差”反映了你的算法性能在训练集上比“最优分类器”差多少。

方差的定义和之前一样。理论上，我们能在巨大的训练集上一直减少方差直至接近0。因此，所有的方差都是在有足够大
的数据集时“可避免的”，所有并没有“不可避免的方差”这种东西。

再考虑一个例子，其中最优误差率是14%，并且我们有：

- 训练集误差=15%

- 开发集误差=16%

有别于前面章节我们把这个叫做高偏差的分类器，现在我们能说来自可避免的偏差是1%，方差的误差是1%。因此，这个算法已经
表现得足够好了，很少的空间再去提升。它只比最好误差率差了2%。

我们从这些例子中能了解最优的误差率能够给我们下一步的指引。从统计学上看，最优误差率也叫做贝叶斯误差率，或贝叶斯率。

我们怎么知道最优误差率是多少呢？对于人们已经擅长的认任务来说，例如识别图片或翻译语音片段，你能叫一个人来提供标签
然后测量人类标签和你训练集相关的准确率。这会给出最优误差率的一个估计值。如果你做一个甚至人类都很难解决的问题。
（例如，预测什么电影适合推荐，或者给用户看什么广告）这会很难估计最优误差率。

在“和人类水平的性能相比”章节（章节33到35），我会更详细的讨论学习算法性能和人类水平的性能相比。

在最后的一些章节里，你学会怎么估计可避免的和不可避免的偏差和方差，通过看训练集和开发集误差率。下一个章节会讨论
你怎么使用这些小窍门来把减少偏差的技巧和减少方差的技巧按优先顺序排好。有很多你需要应用的技巧取决于你现在的问题
是高（可避免的）偏差或高方差。继续读吧！！

